{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract zip files\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "files = 'docs.zip'\n",
    "\n",
    "with ZipFile(files,'r') as zip:\n",
    "    zip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7821ce09",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(klasor_adi, ext_arr, k, b, r, similarity_threshold):\n",
    "    file_count = 0\n",
    "    for path in os.listdir(klasor_adi):\n",
    "        if os.path.isfile(os.path.join(klasor_adi, path)):\n",
    "            file_count += 1\n",
    "    \n",
    "    shingles = [] # stored as words\n",
    "    for f in range(0,file_count):\n",
    "        file = open(klasor_adi + \"/\" + str(1000+f) + \".txt\", \"r\").read()\n",
    "        \n",
    "        if ',' in file:\n",
    "            file = file.replace(',', '')\n",
    "        if '.' in file:\n",
    "            file = file.replace('.', '')\n",
    "        if ';' in file:\n",
    "            file = file.replace(';', '')\n",
    "        if ':' in file:\n",
    "            file = file.replace(':', '')\n",
    "        if '!' in file:\n",
    "            file = file.replace('!', '')\n",
    "        if '?' in file:\n",
    "            file = file.replace('?', '')\n",
    "        if '\\n\\n' in file:\n",
    "            file = file.replace('\\n\\n', ' ')\n",
    "        if '\\n' in file:\n",
    "            file = file.replace('\\n', ' ')\n",
    "        \n",
    "        # Shingling\n",
    "        file = file.lower()\n",
    "        arr = file.split(\" \")\n",
    "        k_words = []\n",
    "        for i in range(len(arr)-k):   \n",
    "            s = \"\"\n",
    "            for j in range(k):\n",
    "                s += \" \" + arr[i+j]\n",
    "            k_words.append(s[1:len(s)])\n",
    "        shingles.append(k_words)\n",
    "    \n",
    "    if len(ext_arr) != 0: # for sorgu.txt\n",
    "        shingles.append(ext_arr)\n",
    "    \n",
    "    # Creating the input matrix (shingles x documents)\n",
    "    unique_shingles = []\n",
    "    for shingle_list in shingles:\n",
    "        for shingle in shingle_list:\n",
    "            if shingle not in unique_shingles:\n",
    "                unique_shingles.append(shingle)\n",
    "    \n",
    "    input_matrix = []\n",
    "    for shingle_list in shingles:\n",
    "        shingles_01 = {shingle: 1 if shingle in shingle_list else 0 for shingle in unique_shingles}\n",
    "        input_matrix.append(list(shingles_01.values()))\n",
    "    \n",
    "    # Min Hashing\n",
    "    hashes = []\n",
    "    n_shingles = np.arange(1,len(input_matrix[0])+1)\n",
    "    np.random.shuffle(n_shingles)\n",
    "    arr = n_shingles\n",
    "    for i in range(b*r):\n",
    "        hashes.append(copy.deepcopy(arr))\n",
    "        np.random.shuffle(arr)\n",
    "    \n",
    "    signature_matrix = []\n",
    "    for min_hash in hashes:#b x r\n",
    "        one_signature = []\n",
    "        for doc in range(len(input_matrix)):\n",
    "            for h in range(1,len(min_hash)):\n",
    "                if input_matrix[doc][(np.where(min_hash == h))[0][0]] > 0:\n",
    "                    one_signature.append(h)\n",
    "                    break\n",
    "        signature_matrix.append(one_signature)\n",
    "    \n",
    "    # LSH\n",
    "    buckets = {}\n",
    "    doc_id = 0\n",
    "    for column in range(len(signature_matrix[0])):\n",
    "        band_del = 0 # band delimeter to seperate bands\n",
    "        for band in range(b):\n",
    "            band_values = tuple(signature_matrix[row][column] for row in range(band_del, band_del + r))\n",
    "            band_del += r\n",
    "\n",
    "            bucket_id = \"\".join(map(str, band_values))\n",
    "\n",
    "            if bucket_id in buckets:\n",
    "                buckets[bucket_id].append(doc_id)\n",
    "            else:\n",
    "                buckets[bucket_id] = [doc_id]\n",
    "        doc_id += 1\n",
    "    \n",
    "    return buckets, signature_matrix, similarity_threshold, doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a933ade",
   "metadata": {},
   "source": [
    "## LSH Query to Find Similar Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(input_file):\n",
    "    k = 3\n",
    "    file = open(input_file, \"r\").read()\n",
    "        \n",
    "    if ',' in file:\n",
    "        file = file.replace(',', '')\n",
    "    if '.' in file:\n",
    "        file = file.replace('.', '')\n",
    "    if ';' in file:\n",
    "        file = file.replace(';', '')\n",
    "    if ':' in file:\n",
    "        file = file.replace(':', '')\n",
    "    if '!' in file:\n",
    "        file = file.replace('!', '')\n",
    "    if '?' in file:\n",
    "        file = file.replace('?', '')\n",
    "    if '\\n\\n' in file:\n",
    "        file = file.replace('\\n\\n', ' ')\n",
    "    if '\\n' in file:\n",
    "        file = file.replace('\\n', ' ')\n",
    "\n",
    "    # Shingling\n",
    "    file = file.lower()\n",
    "    arr = file.split(\" \")\n",
    "    k_words = []\n",
    "    for i in range(len(arr)-k):   \n",
    "        s = \"\"\n",
    "        for j in range(k):\n",
    "            s += \" \" + arr[i+j]\n",
    "        k_words.append(s[1:len(s)]) \n",
    "    \n",
    "    buckets, signature_matrix, sim_thres, this_doc = lsh(\"docs\",k_words,k,5,2,0.4)\n",
    "    sim_buckets = []\n",
    "    sim_docs = []\n",
    "    for bucket_id, documents in buckets.items():\n",
    "        for doc in documents:\n",
    "            if doc == this_doc:\n",
    "                sim_buckets.append(documents)\n",
    "                \n",
    "    for i in range(len(sim_buckets)):\n",
    "        for j in range(len(sim_buckets[i])):\n",
    "            if (sim_buckets[i][j] != this_doc) & (sim_buckets[i][j] not in sim_docs):\n",
    "                sim_docs.append(sim_buckets[i][j])\n",
    "\n",
    "    similarity = {}\n",
    "    input_sig = tuple(signature_matrix[row][this_doc-1] for row in range(len(signature_matrix)))\n",
    "    for similar in sim_docs:\n",
    "        signature = tuple(signature_matrix[row][similar] for row in range(len(signature_matrix)))\n",
    "        \n",
    "        # Jaccard Similarity\n",
    "        union = 0\n",
    "        inters = 0\n",
    "        for value in range(len(input_sig)):\n",
    "            if (input_sig[value] != 0) | (signature[value] != 0):\n",
    "                union += 1\n",
    "                if input_sig[value] == signature[value]:\n",
    "                    inters += 1\n",
    "        similarity[similar] = [float(float(inters)/float(union))]\n",
    "    \n",
    "    sorted_sim = {k: v for k, v in sorted(similarity.items(), key=lambda item: item[1], reverse=True)}\n",
    "    if not sorted_sim:\n",
    "        print(\"Benzer dosya bulunamamıştır.\")\n",
    "    else:\n",
    "        count = 0\n",
    "        for doc_id, jaccard in sorted_sim.items():\n",
    "            count += 1\n",
    "            print(str(1000+int(doc_id)) + \".txt \" + str(jaccard[0]))\n",
    "        print(\"Toplam \" + str(count) + \" dosya bulunmuştur.\")\n",
    "                \n",
    "query(\"sorgu.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim(input_file, k, b, r):\n",
    "    file = open(input_file, \"r\").read()\n",
    "        \n",
    "    if ',' in file:\n",
    "        file = file.replace(',', '')\n",
    "    if '.' in file:\n",
    "        file = file.replace('.', '')\n",
    "    if ';' in file:\n",
    "        file = file.replace(';', '')\n",
    "    if ':' in file:\n",
    "        file = file.replace(':', '')\n",
    "    if '!' in file:\n",
    "        file = file.replace('!', '')\n",
    "    if '?' in file:\n",
    "        file = file.replace('?', '')\n",
    "    if '\\n\\n' in file:\n",
    "        file = file.replace('\\n\\n', ' ')\n",
    "    if '\\n' in file:\n",
    "        file = file.replace('\\n', ' ')\n",
    "\n",
    "    # Shingling\n",
    "    file = file.lower()\n",
    "    arr = file.split(\" \")\n",
    "    k_words = []\n",
    "    for i in range(len(arr)-k):   \n",
    "        s = \"\"\n",
    "        for j in range(k):\n",
    "            s += \" \" + arr[i+j]\n",
    "        k_words.append(s[1:len(s)])\n",
    "    \n",
    "    buckets, signature_matrix, sim_thres, this_doc = lsh(\"docs\",k_words,k,b,r,0.4)\n",
    "    sim_buckets = []\n",
    "    sim_docs = []\n",
    "    for bucket_id, documents in buckets.items():\n",
    "        for doc in documents:\n",
    "            if doc == this_doc:\n",
    "                sim_buckets.append(documents)\n",
    "               \n",
    "    for i in range(len(sim_buckets)):\n",
    "        for j in range(len(sim_buckets[i])):\n",
    "            if (sim_buckets[i][j] != this_doc) & (sim_buckets[i][j] not in sim_docs):\n",
    "                sim_docs.append(sim_buckets[i][j])\n",
    "    \n",
    "    similarity = {}\n",
    "    input_sig = tuple(signature_matrix[row][this_doc-1] for row in range(len(signature_matrix)))\n",
    "    for similar in sim_docs:\n",
    "        signature = tuple(signature_matrix[row][similar] for row in range(len(signature_matrix)))\n",
    "        \n",
    "        # Jaccard Similarity\n",
    "        union = 0\n",
    "        inters = 0\n",
    "        for value in range(len(input_sig)):\n",
    "            if (input_sig[value] != 0) | (signature[value] != 0):\n",
    "                union += 1\n",
    "                if input_sig[value] == signature[value]:\n",
    "                    inters += 1\n",
    "        similarity[similar] = [(inters/union)]\n",
    "    return similarity\n",
    "    \n",
    "def lsh_count(input_file):\n",
    "    # 1\n",
    "    x = [\"k=2\", \"k=3\", \"k=4\"]\n",
    "    y = []\n",
    "    for i in range(2,5):\n",
    "        lsh_ = find_sim(input_file,i,5,4)\n",
    "        y.append(len(lsh_))\n",
    "    \n",
    "    plt.bar(x, y)\n",
    "    plt.xlabel('k values')\n",
    "    plt.ylabel('number of similar documents')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2\n",
    "    x = [\"b=3\", \"b=4\", \"b=5\"]\n",
    "    y = []\n",
    "    for i in range(3,6):\n",
    "        lsh_ = find_sim(input_file,3,i,4)\n",
    "        y.append(len(lsh_))\n",
    "    \n",
    "    plt.bar(x, y)\n",
    "    plt.xlabel('band values')\n",
    "    plt.ylabel('number of similar documents')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3\n",
    "    x = [\"r=3\", \"r=4\", \"r=5\"]\n",
    "    y = []\n",
    "    for i in range(3,6):\n",
    "        lsh_ = find_sim(input_file,2,4,i)\n",
    "        y.append(len(lsh_))\n",
    "    \n",
    "    plt.bar(x, y)\n",
    "    plt.xlabel('row values')\n",
    "    plt.ylabel('number of similar documents')\n",
    "    plt.show()\n",
    "\n",
    "lsh_count(\"sorgu.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
